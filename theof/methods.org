#+OPTIONS: toc:nil ':t H:5
#+STARTUP: hidestars overview

* methods
** preliminary note
With our initial exploratory approach we were able to cover a wide
variety of Vossantos but clearly missed some, as an analysis of some
[[file:../examples/br.org#examples][examples]] shows. Therefore, we thought about simplified patterns which
just match the source, for instance:
#+BEGIN_SRC
  (a|the) (next|new|future|would-be)? ([A-Z][a-z]+ ){1,3} of
#+END_SRC

This pattern would not match the following examples from the NYT
corpus:
- "the Mick Jagger or Michael Jordan of"
- "the Michael Jordan and Magic Johnson of"
- "She could be the Michael Jordan that women's basketball hasn't yet had"
- "He has not been the Michael Jordan in the debates"
- "he was the Babe Ruth, the Michael Jordan, the Wayne Gretzky of racing"
- "if Mel Gibson really were Michelangelo of his generation"
- names which contain lower-case terms (e.g., "van Gogh")
- adjectives that further describe the target, e.g., "the abused
  pint-size Michelangelo of"

This shows the difficulty to find a good balance between recall and
precision. We limited ourselves to the most basic pattern "the
... of", where "..."  represents up to five words which match a
Wikidata entity.

** finding source candidates in the corpus
We search for sentences which contain the pattern
~\\bthe\\s(\\w+\\s+){1,5}?of\\b~:
#+BEGIN_SRC sh
  export PYTHONIOENCODING=utf-8
  for year in $(seq 1987 2007); do
    # searches for \\bthe\\s(\\w+\\s+){1,5}?of\\b
    ./theof.py nyt_corpus_${year}.tar.gz > theof_${year}.tsv
  done
#+END_SRC

Allowing up to five words (both capitalised and not) adds noise, e.g.,
- "the other is the Maggie Smith of"
- "referred to the writer as the Clifford Irving of baseball"
- "the past are the David Brinkley of"
- "the Pope was the Madonna of"
which causes the entities in those strings to not match. After
extending ~check_wikidata.py~, we can run the extraction.

** most frequent candidates
In the NYT corpus we find 575711 hits for 1987 alone, which comprise
163565 phrases, the ten most frequent ones are the following:
#+BEGIN_SRC sh
  # echo hits $(cat theof_1987.tsv | wc -l)
  awk -F'\t' '{print $2}' theof_1987.tsv| sort | uniq -c | sort -nr | head
#+END_SRC

| count | phrase            |
|-------+-------------------|
|  7205 | the end of        |
|  6540 | the University of |
|  4493 | the number of     |
|  3239 | the sale of       |
|  3239 | the rest of       |
|  2451 | the use of        |
|  2233 | the kind of       |
|  2154 | the son of        |
|  1961 | the age of        |
|  1939 | the president of  |

This clearly shows that the approach needs to be improved. Our idea is
to match the source candidates in the phrases against a list of person
names from Wikidata.

** identifying source candidates in Wikidata
This section describes how to extract a list of names from
Wikidata. We focus on items which have the [[https://www.wikidata.org/wiki/Property:P31][instance of]] property [[https://www.wikidata.org/wiki/Q5][human]].

*** item has "instance of" property "human"

SPARQL query to retrieve all instances of [[https://www.wikidata.org/wiki/Q5][human]]:
#+BEGIN_SRC sparql
  SELECT ?item ?itemLabel WHERE
  {
    ?item wdt:P31 wd:Q5 .                  # instance of human
    SERVICE wikibase:label {               # ... include the labels
      bd:serviceParam wikibase:language "en"
    }
  }
#+END_SRC

Dowloading the data:
#+BEGIN_SRC sh :results silent
  curl \
      --header "Accept: text/tab-separated-values" \
      --output wikidata_humans.tsv \
      --globoff \
"https://query.wikidata.org/sparql?query=SELECT%20%3Fitem%20%3FitemLabel%20WHERE%0A%20%20{%0A%20%20%20%20%3Fitem%20wdt%3AP31%20wd%3AQ5%20.%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20instance%20of%20human%0A%20%20%20%20SERVICE%20wikibase%3Alabel%20{%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20...%20include%20the%20labels%0A%20%20%20%20%20%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22%0A%20%20%20%20}%0A%20%20}"
#+END_SRC

Checking the result:
#+BEGIN_SRC sh
  wc -l wikidata_humans.tsv
#+END_SRC

: 139649 wikidata_humans.tsv
Clearly some are missing here!

Let us try to get them without a label:
#+BEGIN_SRC sh :results silent
  curl \
      --header "Accept: text/tab-separated-values" \
      --output wikidata_humans_wo_labels.tsv \
      --globoff \
"https://query.wikidata.org/sparql?query=SELECT%20%3Fitem%20%3FitemLabel%20WHERE%0A%20%20{%0A%20%20%20%20%3Fitem%20wdt%3AP31%20wd%3AQ5%20.%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20instance%20of%20human%0A%20%20}"
#+END_SRC

#+BEGIN_SRC sh
  wc -l wikidata_humans_wo_labels.tsv
#+END_SRC

: 3455229 wikidata_humans_wo_labels.tsv

This looks much better. Is there a way to get a label for all of them?

With the [[https://www.mediawiki.org/wiki/Wikidata_Toolkit][Wikidata Toolkit]] and some Java code:
#+BEGIN_SRC java
  public void processItemDocument(final ItemDocument itemDocument) {
      // find an instance of "human"
      if (itemDocument.hasStatementValue("P31", Datamodel.makeWikidataItemIdValue("Q5"))) {
          final ItemIdValue itemId = itemDocument.getItemId();
          final MonolingualTextValue label = itemDocument.getLabels().get("en");
          if (label != null) {
              buf.write(csvEscape(itemId.getId()) + "\t" + csvEscape(label.getText()));
              // add aliases
              final List<MonolingualTextValue> aliases = itemDocument.getAliases().get("en");
              if (aliases != null) {
                  for (final MonolingualTextValue alias : aliases) {
                      buf.write("\t" + csvEscape(alias.getText()));
                  }
              }
              buf.write("\n");
          }
      }
  }
#+END_SRC
it is possible:
: Found 3454611 matching items after scanning 26132045 items.

Of those, 2801931 have a label:
#+BEGIN_SRC sh
  wc -l wikidata_humans.tsv
#+END_SRC

: 2801931 wikidata_humans.tsv

With the Java code we also extracted aliases for the names (e.g.,
"Mozart" for [[https://www.wikidata.org/wiki/Q254][Wolfgang Amadeus Mozart]]) to enable matching against
non-complete names.

We will use that file in the sequel.

*** "fictional character"
Later, we would like to include [[https://www.wikidata.org/wiki/Q95074][fictional character]]. We can retrieve
all instances of any subclass of fictional character with this SPARQL
query:
#+BEGIN_SRC sparql
  SELECT ?item ?itemLabel WHERE
  {
    ?item (wdt:P31/wdt:P279*) wd:Q95074. # instance of any subclass of fictional character
    SERVICE wikibase:label { bd:serviceParam wikibase:language "en". }
  }
#+END_SRC

#+BEGIN_SRC sh :results silent
  curl \
      --header "Accept: text/tab-separated-values" \
      --output wikidata_fictional_characters.tsv \
      --globoff \
"https://query.wikidata.org/sparql?query=SELECT%20%3Fitem%20%3FitemLabel%20WHERE%20{%0A%20%20%3Fitem%20%28wdt%3AP31%2Fwdt%3AP279*%29%20wd%3AQ95074.%0A%20%20SERVICE%20wikibase%3Alabel%20{%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22.%20}%0A}"
#+END_SRC

#+BEGIN_SRC sh
  wc -l wikidata_fictional_characters.tsv
#+END_SRC

: 46227 wikidata_fictional_characters.tsv
Seems reasonable.

** matching the phrases against the Wikidata candidates

We experiment first with the data from 1987 and match all sources in
the "the ... of" phrases against the Wikidata list of humans:
#+BEGIN_SRC sh
  export PYTHONIOENCODING=utf-8
  ./check_wikidata.py theof_1987.tsv.gz wikidata_humans.tsv > theof_1987_wd.tsv
#+END_SRC

How many (distinct) phrases do remain?
#+BEGIN_SRC sh
  wc -l theof_1987_wd.tsv
  awk -F'\t' '{print $3}' theof_1987_wd.tsv | sort -u | wc -l
#+END_SRC

| 2880 | theof_1987_wd.tsv |
|  371 | distinct phrases  |

Which are the most frequent ones?
#+BEGIN_SRC sh
  awk -F'\t' '{print $3}' theof_1987_wd.tsv | sort | uniq -c | sort -nr | head
#+END_SRC

| freq | phrase          |
|------+-----------------|
|  764 | the House of    |
|  316 | the wife of     |
|  225 | the Church of   |
|  216 | the Hall of     |
|  122 | the Bill of     |
|   83 | the Freedom of  |
|   77 | the Governor of |
|   59 | the Duke of     |
|   53 | the King of     |
|   45 | the Sultan of   |
|      |                 |
So most of those are covered by rather unusual "names".

** blacklisting names
We put such rather unusual "names" into a [[file:blacklist.tsv][blacklist]]:
#+BEGIN_SRC sh
  awk -F'\t' '{print $3}' theof_1987_wdfiltered.tsv | sort | uniq -c | sort -nr > blacklist.tsv
#+END_SRC
which we manually clean up.

Now we repeat the analysis:
#+BEGIN_SRC sh :results silent
  export PYTHONIOENCODING=utf-8
  ./check_wikidata.py --blacklist blacklist.tsv theof_1987.tsv wikidata_humans.tsv > theof_1987_wd_bl.tsv
#+END_SRC

How many results did we get?
#+BEGIN_SRC sh
  wc -l theof_1987_wd_bl.tsv
  awk -F'\t' '{print $3}' theof_1987_wd_bl.tsv | sort -u | wc -l
#+END_SRC

| 104 | theof_1987_wd_bl.tsv |
|  88 |                      |

Which were the most frequent ones?
#+BEGIN_SRC sh
  awk -F'\t' '{print $3}' theof_1987_wd_bl.tsv | sort | uniq -c | sort -nr | head
#+END_SRC

| freq | phrase                 |
|------+------------------------|
|    4 | the Horatio Alger of   |
|    4 | the Frank Sinatra of   |
|    3 | the Woody Allen of     |
|    3 | the Madonna of         |
|    2 | the Tom Seaver of      |
|    2 | the Pete Rose of       |
|    2 | the Joan Baez of       |
|    2 | the Jackie Robinson of |
|    2 | the Groucho Marx of    |
|    2 | the Abraham Lincoln of |

Much better! Let us repeat this process for all other years.

*** TODO criteria
Since the blacklist is quite long
#+BEGIN_SRC sh
  wc -l blacklist.tsv
#+END_SRC

: 1290 blacklist.tsv

it is worthwhile to describe how we created it: We have iteratively
examined the source candidates for the years and added names to the
blacklist which were not obviously (famous) people. This mainly
affected names with just one word
#+BEGIN_SRC sh
  grep " " blacklist.tsv | wc -l
#+END_SRC

: 62
which often are also English nouns like house, hall, church, bill.

** processing all remaining years
*** matching against Wikidata without blacklist
#+BEGIN_SRC sh :results silent
  export PYTHONIOENCODING=utf-8
  for year in $(seq 1987 2007); do
    ./check_wikidata.py theof_${year}.tsv.gz wikidata_humans.tsv > theof_${year}_wd.tsv
  done
#+END_SRC
Now we have restricted the list to phrases which contain a source that
matches a Wikidata item with an "instance of" property of "human".

*** extending the blacklist
**** Step 1
We create an initial blacklist using the terms from the first year:
#+BEGIN_SRC sh :results silent
  export PYTHONIOENCODING=utf-8
  ./check_wikidata.py theof_1987.tsv wikidata_humans.tsv |\
      awk -F'\t' '{print $4}' | sort | uniq -c | sort -nr > bl.tsv
#+END_SRC

**** Step 2
Now we edit ~bl.tsv~ to remove all entities which are OK ... and then
we cleanup and append to the blacklist which we then commit:
#+BEGIN_SRC sh :results silent
  sed -e "s/^ *//" -e "s/ /\t/" bl.tsv | awk -F'\t' '{print $2"\t"$1}' >> blacklist.tsv
#  git commit blacklist.tsv -m"+1987"
#+END_SRC

**** Step 3
We extract new candidates for our blacklist using the next year:
#+BEGIN_SRC sh :results silent
  export PYTHONIOENCODING=utf-8
  # current year (increment)
  YEAR=1988
  LAST_YEAR=$((YEAR-1))

  # use the blacklist we had created previously to create a
  # blacklist-cleaned file
  ./check_wikidata.py -b blacklist.tsv theof_${LAST_YEAR}.tsv wikidata_humans.tsv > theof_${LAST_YEAR}_wd_bl.tsv

  # extend the blacklist by all previous positive terms (such that we do
  # not have to remove them again)
  cp blacklist.tsv blacklist_lybl.tsv
  for iy in $(seq 1987 $LAST_YEAR); do
      awk -F'\t' '{print $3}' theof_${iy}_wd_bl.tsv | sort | uniq -c | sort -nr | \
          sed -e "s/^ *//" -e "s/ the /\t/" -e "s/ of$//" | awk -F'\t' '{print $2"\t"$1}' \
                                                                >> blacklist_lybl.tsv
  done

  # now we extract only the new hits
  ./check_wikidata.py -b blacklist_lybl.tsv theof_${YEAR}.tsv wikidata_humans.tsv |\
      awk -F'\t' '{print $3}' | sort | uniq -c | sort -nr > bl.tsv
#+END_SRC

... and repeat this for the next year (goto Step 2).

** format data for analysis
Put everything into this org file:
#+BEGIN_SRC sh :results silent
  export PYTHONIOENCODING=utf-8
  rm newR.org
  for year in $(seq 1987 2007); do
    echo "**" $year >> newR.org
    ./tsv2org.py theof_${year}_wda_bl.tsv >> newR.org
    echo >> newR.org
  done
#+END_SRC

Merge with existing ~results.org~:
#+BEGIN_SRC sh
  ./org.py --merge newR.org results.org  > new_results.org
#+END_SRC

** TODO manual post-processing
- detection of false positives
  - how?
- repairing Wikidata links
  - e.g., from [[https://www.wikidata.org/wiki/Q4865108][Bart Simpson, the Canadian film director]], to [[https://www.wikidata.org/wiki/Q5480][Bart
    Simpson, the fictional character from The Simpsons franchise]], or
    from [[https://www.wikidata.org/wiki/Q62095][Johann Andreas Wagner]] to [[https://www.wikidata.org/wiki/Q1511][Richard Wagner]] (the former was
    selected because his surname is provided as "Also known as"
    alternative in Wikidata)
- repairing markup of source
- completing some sentences that got broken due to NLTK sentence
  tokenisation (e.g., for 1988/06/05/0151262, 1991/02/10/0421854)
- some sentences contained a Vossanto, but the wrong part was
  identified, e.g., in  1998/12/20/1071267: /''I am not the Peron of
  Venezuela, I am the Chavez of Venezuela,'' he has said./ we have
  identified "the Chavez of Venezuela", though that is not a Vossanto
  (but "the Peron of Venezuela" is one) - those are marked negatived
  with a "(W)"
- some sentences contain several Vossantos where just one was
  identified, e.g., "WHEN the Godzilla of the Internet decides to wed
  *the King Kong of* /content/"

- handling of duplicates, types of duplicates:
  - exact same sentence but different article
  - same phrase/similar sentence in same/different article
- adding markup for modifier

- TODO :: For sentences containing several Vossantos only one source
          is extracted. This means that some sources can not be
          counted properly.

*** TODO markup for false positives

| symbol | explanation                                            |
|--------+--------------------------------------------------------|
| (W)    | sentence contains Vossanto but *wrong* phrase detected |
| (D)    | duplicate                                              |

*** TODO duplicates that need to be handled

#+BEGIN_SRC sh :results raw
  ./org.py -T -t -c results.org | sort | uniq -c | sort -nr | sed "s/^ */- /" | grep -v "^- 1 " | less -S
#+END_SRC

- 2 When Robert accuses her of being *the Jerry Bruckheimer of* cakes, she responds that he's *the Paul Thomas Anderson of* cakes.
- 2 ''She's like *the Stephen King of* our genre,'' Ms. Lynch said, and then amended that to ''Stephen King is *the Nora Roberts of* his genre,'' as her friend and writing partner, Cai Smith, from Wisconsin, nodded her head in agreement.
- 2 She is *the Robert Ryman of* the 21st century.
- 2 Mr. Reagan fares far better, nicknamewise, than some other Presidents in the compendium, including one known as Gloomy Gus, King Richard, *the Bela Lugosi of* American Politics, Richard the Chicken-Hearted, *the Nero of* Our Times, the Tarnished President, the Godfather, St. Richard the Commie Killer, President Truthful and Trickie Dick.
- 2 Marly's grandly histrionic mother, Delia Temple (Jane Alexander), known as ''*the Sarah Bernhardt of* Delrona Beach,'' runs the community theater and is a devout environmentalist.
- 2 Imagine *the Grace Kelly of* ''Rear Window'' or *the Kim Novak of* ''Vertigo'' sprawled seductively on an analyst's couch, smoking cigarettes and confiding her sexual frustration to a repressed, wide-eyed shrink who is obsessed with her.
- 2 If Ms. Clayburgh was *the Julia Roberts of* the late 1970's, Gabriel Byrne might be described as *the Humphrey Bogart of* the early 1990's.
- 2 ''I felt that he was *the Franklin Delano Roosevelt of* 1972,'' Mr.
- 2 He described Metro-North, with its many service awards, as ''*the Tom Hanks of* commuter railroads.''
- 2 Deeply committed to his faith -- he taught adult classes most Sundays at *the Frederick Church of* the Brethren -- the Iron Man was determined to become a philanthropist.
- 2 By STEPHEN HOLDEN              Meld *the Lucille Ball of* ''I Love Lucy'' with *the Melanie Griffith of* ''Working Girl,'' and you have Judy Holliday, the brilliant comedian who redefined the stereotype of the dumb blonde as ''the smart dumb blonde.''
- 2 And there's *the Harold Stassen of* New York, Herman Badillo, and *the Paul Tsongas of* New York, Richard Ravitch.
- 2 40 and 41 jump into the business at hand, with a suddenness that foreshadows *the Beethoven of* the Eighth Symphony and last string quartet.

