#!/usr/bin/python3
# -*- coding: utf-8 -*-

#
# Process (extract, filter, merge) Vossantos in an org mode file.
#
# Usage: Without any arguments, extracts all Vossanto canidates from
# the given org file.
#
# Author: rja
#
# Changes:
# 2018-10-08 (rja)
# - added option "-c" to output classification (True/False) and renamned existing "-c" to "-C"
# 2018-09-11 (rja)
# - normalising "None" to "" in output
# 2018-08-22 (rja)
# - added extraction of status that explains false positives
# 2018-08-21 (rja)
# - added extraction of source phrase as it appears in the text
# - key uses source phrase instead of source label and ignores modifier markup ('/')
# - key now uses all characters from sentence (not just the first 40)
# 2018-08-16 (rja)
# - added date extraction
# 2018-08-15 (rja)
# - improved modifier extraction
# 2018-08-09 (rja)
# - added option -o to output modifier
# 2018-03-02 (rja)
# - added option -U to include article URLs from file
# - added option -u to extract article URLs
# - annotated line regexp
# 2017-05-17 (rja)
# - simplified file parameters to support reading from STDIN
# 2017-05-14 (rja)
# - added options for selection
# 2017-05-13 (rja)
# - renamed from mergeorg.py and extended for extraction and filtering
# - migrated to Python3
# 2017-05-11 (rja)
# - initial version

import re
import argparse
import sys
from collections import OrderedDict

version = "0.0.6"

# 1. [[https://www.wikidata.org/wiki/Q83484][Anthony Quinn]] (1987/01/02/0000232) ''I sometimes feel like *the Anthony Quinn of* my set.''
line_re_str = """
^                     # beginning of string
(?P<newmark>> )?      # new candidates are marked with "> "line
(?P<id>[0-9]+)\.      # all candidates are numbered
[ !]+                 # space and/or !
\+?                   # modifier for false positive
\[\[.+/               # start of Wikidata URL
(?P<wdid>[^/]+)       # Wikidata id
\]\[                  # separators
(?P<wdlabel>.+)       # Wikidata label
\]\]                  # end of Wikidata URL
\                     # space
\(                    # beginning of file id
(?P<article>          # beginning of full article part
(\[\[)?               # opening markup for article URL
(?P<aurl>http.+?)?    # article URL
(\]\[)?               # separators for article URL
(?P<fid>              # full file id
(?P<year>\\d{4})      # year
/                     # separator
(?P<month>\\d{2})     # month
/                     # separator
(?P<day>\\d{2})       # day
/                     # separator
(?P<aid>\\d+)         # article id
)                     # end of full file id
(\]\])?               # closing markup for article URL
)                     # end of full article part
\)                    # end of file id
\                     # space
(?P<sentence>.+?[^+]) # sentence
(?P<truefalse>\+)?    # false positive indicator
(\ \(                 # beginning of status token explaining false positives
(?P<status>[WD]+)     # a combination of characters
\))?                  # end of (optional) token
$                     # end of string
"""
re_line = re.compile(line_re_str, re.VERBOSE)

# to extract the modifier (enclosed in /.../) from the sentence
re_modifier = re.compile("of\\* ['\"]*/(.+?)/([^0-9A-Za-z]|$)")

# to extract the exact source phrase (enclosed in *the ... of*) from the sentence
re_sourcephrase = re.compile("\\*the (.+?) of\\*")

# to remove markup from the sentences
re_clean = re.compile(r"[*/.\s]")

# remove line breaks and tabs from text
re_ws = re.compile('[\n\t\r]+')


# reads the file into which the other file shall be merged
# all non-vossanto lines are returned in lines,
# all following (vossanto) lines are indexed in index using
# a key generated by match_line
def read_file(flines):
    lines = []
    index = None
    for line in flines:
        # different handling for lines before and after the heading
        if line.startswith("* results"):
            index = dict()
            lines.append(line)
        else:
            if index is not None:
                # index lines after heading "* results"
                parts = match_line(line)
                if parts:
                    year, key = get_key(parts)
                    if year not in index:
                        index[year] = dict()
                    index[year][key] = line
            else:
                # store lines before heading "* results"
                lines.append(line)
    return lines, index

# reads a TSV file with article ids and corresponding URLs
def read_urls(flines):
    urls = dict()
    for line in flines:
        aid, url = line.strip().split('\t')
        urls[aid] = url
    return urls

def gen_truefalse(candidates, true_positive, false_positive):
    for year, date, aid, fid, aurl, sourceId, sourceLabel, sourcePhrase, modifier, sentence, trueVoss, newVoss, status in candidates:
        if true_positive == false_positive or true_positive == trueVoss or false_positive != trueVoss:
            yield year, date, aid, fid, aurl, sourceId, sourceLabel, sourcePhrase, modifier, sentence, trueVoss, newVoss, status

def gen_candidates(lines):
    for line in lines:
        parts = match_line(line)
        if parts:
            yield parts

# remove control characters
def gen_rm_ctrl(parts):
    for part in parts:
        yield [re_ws.sub(' ', p).strip() for p in part]

# generates a key for a Vossanto
def get_key(parts):
    year, date, aid, fid, aurl, sourceId, sourceLabel, sourcePhrase, modifier, sentence, trueVoss, newVoss, status = parts
    key = "|".join([year, aid, sourcePhrase, re_clean.sub('', sentence)])
    return year, key

def select_parts(parts, syear, sdate, said, sfid, saurl, ssourceId, ssourceLabel, ssourcePhrase, smodifier, stext, swikidata, sstatus, sclassification):
    if any([syear, sdate, said, sfid, saurl, ssourceId, ssourceLabel, ssourcePhrase, smodifier, stext, swikidata, sstatus]):
        for year, date, aid, fid, aurl, sourceId, sourceLabel, sourcePhrase, modifier, sentence, trueVoss, newVoss, status in parts:
            result = []
            if syear:
                result.append(year)
            if sdate:
                result.append(date)
            if said:
                result.append(aid)
            if sfid:
                result.append(fid)
            if ssourceId:
                result.append(sourceId)
            if ssourceLabel:
                result.append(sourceLabel)
            if ssourcePhrase:
                result.append(sourcePhrase)
            if smodifier:
                result.append(modifier)
            if stext:
                result.append(sentence)
            if swikidata:
                result.append("[[https://www.wikidata.org/wiki/" + sourceId + "][" + sourceLabel + "]]")
            if saurl:
                result.append(aurl)
            if sstatus:
                result.append(status)
            if sclassification:
                result.append(trueVoss)
            yield result
    else:
        # when nothing has been selected, return everything
        for part in parts:
            yield part

# checks if the line is a Vossanto line
def match_line(line):
    # detect the Vossanto lines
    match = re_line.match(line.strip())
    if match:
        d = match.groupdict()
        newVoss = d["newmark"]
        sourceId = d["wdid"]
        sourceLabel = d["wdlabel"]
        fid = d["fid"]
        year = d["year"]
        date = d["year"] + "-" + d["month"] + "-" + d["day"]
        aid = d["aid"]
        aurl = d["aurl"]
        sentence = d["sentence"]
        trueVoss = d["truefalse"] != "+"
        status = d["status"]
        # extract from sentence
        sourcePhrase = extract_sourcephrase(sentence)
        modifier = extract_modifier(sentence, trueVoss)
        return year, date, aid, fid, aurl, sourceId, sourceLabel, sourcePhrase, modifier, sentence, trueVoss, newVoss, status
    return None

# extract the modifier (enclosed in /.../) from the sentence
def extract_modifier(sentence, trueVoss):
    # ignore non-Vossantos
    if trueVoss:
        match = re_modifier.search(sentence)
        if match:
            return match.group(1)
    return ""

# extract the source phrase (enclosed in *the ... of*) from the sentence
def extract_sourcephrase(sentence):
    match = re_sourcephrase.search(sentence)
    if match:
        return match.group(1)
    return ""


# given a line, either adds the URL for the article or (if already existent), changes it
def set_article_url(line, urls):
    # detect Vossanto line
    match = re_line.match(line.strip())
    if match:
        d = match.groupdict()
        fid = d["fid"]
        if fid not in urls:
            print("WARN: URL for", fid, "not found", file=sys.stderr)
        else:
            url = urls[fid]
            # implement
            article = d["article"]
            return line.replace(article, "[[" + url + "][" + fid + "]]")
    else:
        print("WARN: line did not match", line[:50], file=sys.stderr)

# inserts a vossanto line into the index
def insert(index, line, string_new = '> '):
    # extract key for this line
    parts = match_line(line)
    if not parts:
        # print warning only if not a year heading
        if not re.match("^\*{2,3} [0-9]{4}$", line.strip()):
            print("WARN: line did not match", line[:50], file=sys.stderr)
        return
    # add new Vossanto
    year, key = get_key(parts)
    if key not in index[year]:
        index[year][key] = string_new + line

# convert value to string, taking care of None
def part_to_string(p):
    if p is None:
        return ""
    return str(p)
    

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Merge Vossantos in org files.', formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('file', type=argparse.FileType('r', encoding='utf-8'), nargs='?', default=sys.stdin, help='org mode file to process')
    # what shall be printed
    parser.add_argument('-a', '--articleid', action="store_true", help="output article id")
    parser.add_argument('-b', '--status', action="store_true", help="output status of false positives")
    parser.add_argument('-c', '--classification', action="store_true", help="output classification (True/False)")
    parser.add_argument('-d', '--date', action="store_true", help="output date")
    parser.add_argument('-f', '--fileid', action="store_true", help="output file id")
    parser.add_argument('-i', '--sourceid', action="store_true", help="output Wikidata source id")
    parser.add_argument('-l', '--sourcelabel', action="store_true", help="output source")
    parser.add_argument('-o', '--modifier', action="store_true", help="output modifier")
    parser.add_argument('-p', '--sourcephrase', action="store_true", help="output source phrase") # as it appears in the text
    parser.add_argument('-t', '--text', action="store_true", help="output text")
    parser.add_argument('-u', '--url', action="store_true", help="output article URL")
    parser.add_argument('-w', '--wikidata', action="store_true", help="output link to Wikidata")
    parser.add_argument('-y', '--year', action="store_true", help="output year")
    # other options
    parser.add_argument('-m', '--merge', type=argparse.FileType('r', encoding='utf-8'), metavar="FILE", help='file to merge')
    parser.add_argument('-n', '--new', type=str, metavar="S", help="string to mark new entries", default='> ')
    parser.add_argument('-T', '--true', action="store_true", help="output only true Vossantos")
    parser.add_argument('-F', '--false', action="store_true", help="output only false positives")
    parser.add_argument('-C', '--clean', action="store_true", help="clean whitespace")
    parser.add_argument('-s', '--separator', type=str, metavar="SEP", help="output separator", default='\t')
    parser.add_argument('-U', '--include-urls', type=argparse.FileType('r', encoding='utf-8'), metavar="FILE", help='file with article URLs')
    parser.add_argument('-v', '--version', action="version", version="%(prog)s " + version)

    args = parser.parse_args()

    if args.merge:
        # read file into which the other file shall be merged
        lines, index = read_file(args.file)

        # read new file and insert Vossantos
        for line in args.merge:
            insert(index, line, args.new)
        # print first (unchanged) part of original file
        for line in lines:
            print(line, end='')

        # print Vossanto lines
        for year in sorted(index):
            print()
            print("**", year)
            for line in sorted(index[year]):
                print(index[year][line], end='')
    elif args.include_urls:
        # read URL file
        urls = read_urls(args.include_urls)
        # read file
        lines, index = read_file(args.file)
        # print first (unchanged) part of original file
        for line in lines:
            print(line, end='')
        # print Vossanto lines
        for year in sorted(index):
            print()
            print("**", year)
            for line in sorted(index[year]):
                # add URL to line
                print(set_article_url(index[year][line], urls), end='')
    else:
        # default: extract Vossantos
        parts = gen_candidates(args.file)
        parts = gen_truefalse(parts, args.true, args.false)
        parts = select_parts(parts, args.year, args.date, args.articleid, args.fileid, args.url, args.sourceid, args.sourcelabel, args.sourcephrase, args.modifier, args.text, args.wikidata, args.status, args.classification)
        if args.clean:
            parts = gen_rm_ctrl(parts)
        for part in parts:
            print(args.separator.join([part_to_string(p) for p in part]))
